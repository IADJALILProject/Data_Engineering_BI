# Pipeline d'Ingénierie de Données pour l'Analyse Bancaire

## Introduction
Ce projet propose une solution complète d'**ingénierie des données** pour le **secteur bancaire**, permettant l'analyse approfondie des domaines critiques comme les **transactions**, les **crédits**, les **fraudes** et les **profils clients**. 

Le pipeline repose sur :
- Une **architecture en étoile** pour une structuration optimisée des données.
- Une **orchestration automatisée** avec **Prefect**.
- Un déploiement conteneurisé avec **Docker**.

## Objectifs du Projet
- **Ingestion** : Automatiser la collecte et le prétraitement des données.
- **Transformation** : Structurer les données pour l'analyse.
- **Stockage** : Assurer un stockage fiable dans **PostgreSQL** et **Delta Lake**.
- **Monitoring** : Contrôler la qualité des données.
- **Orchestration** : Planifier et exécuter les workflows via Prefect.
- **Tests** : Valider la fiabilité des scripts grâce à des tests unitaires.

---

## Architecture du Pipeline

### Modèle en étoile
Le pipeline suit un modèle en étoile pour optimiser les analyses :

#### Tableaux de Faits :
- **fact_transactions** : Historique des transactions bancaires.
- **fact_loans** : Informations sur les crédits octroyés.
- **fact_frauds** : Activités frauduleuses identifiées.

#### Tableaux de Dimensions :
- **dim_clients** : Informations sur les clients.
- **dim_dates** : Structure temporelle.
- **dim_accounts** : Détails des comptes clients.

---

## Structure du Projet

```
my_banking_pipeline/
├── data/
│   ├── raw/            # Données brutes collectées
│   ├── processed/      # Données nettoyées
│   └── transformed/    # Données prêtes pour l'analyse
├── src/
│   ├── ingestion/      # Scripts d'ingestion des données
│   ├── transformation/ # Scripts de transformation des données
│   ├── storage/        # Scripts pour le stockage dans PostgreSQL/Delta Lake
│   └── orchestration/  # Orchestration via Prefect
├── tests/              # Tests unitaires
├── docker-compose.yml  # Configuration Docker
├── README.md           # Documentation du projet
└── requirements.txt    # Liste des dépendances Python
```

---

## Installation

### Prérequis
- Python 3.8+
- Docker
- PostgreSQL
- Prefect

### Étapes d'Installation

1. Clonez ce dépôt :
   ```bash
   git clone git@github.com:username/banking_data_pipeline.git
   cd banking_data_pipeline
   ```

2. Installez les dépendances :
   ```bash
   pip install -r requirements.txt
   ```

3. Configurez les fichiers de connexion (exemple : `config.yaml`) :
   ```yaml
   database:
     host: localhost
     user: postgres
     password: your_password
     dbname: banking_db
   ```

4. Déployez les conteneurs Docker :
   ```bash
   docker-compose up
   ```

---

## Utilisation

### 1. Exécution du pipeline
Pour lancer le pipeline complet :
```bash
python src/orchestration/orchestration.py
```

### 2. Tests unitaires
Lancez les tests unitaires pour vérifier la qualité des différents modules :
```bash
pytest tests/
```

### 3. Planification
Configurez Prefect pour exécuter automatiquement les workflows à des intervalles réguliers :
- Exemple de planification quotidienne à 08h00 :
```bash
prefect deployment run --name "Daily Pipeline" --cron "0 8 * * *"
```

---

## Conclusion

Ce projet fournit une solution robuste et automatisée pour le traitement des données critiques dans le secteur bancaire. 

**Points forts :**
- **Modularité** : Facile à maintenir et à étendre.
- **Scalabilité** : Supporte de gros volumes de données.
- **Automatisation** : Gestion des workflows et planification intuitive.

**Prochaines étapes :**
- Ajouter un tableau de bord BI pour visualiser les analyses.
- Intégrer des API pour enrichir les données.

