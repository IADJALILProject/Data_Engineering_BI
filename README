Pipeline d'IngÃ©nierie de DonnÃ©es pour l'Analyse Bancaire

ğŸš€ Description du Projet

Ce projet propose une solution complÃ¨te d'ingÃ©nierie des donnÃ©es pour le secteur bancaire, permettant l'analyse approfondie des domaines critiques comme les transactions, les crÃ©dits, les fraudes et les profils clients. GrÃ¢ce Ã  une architecture en Ã©toile, une orchestration automatisÃ©e avec Prefect, et un dÃ©ploiement via Docker, ce pipeline garantit robustesse, scalabilitÃ© et efficacitÃ©.

ğŸ¯ Objectifs du Projet

Ingestion : Automatiser la collecte et le prÃ©traitement des donnÃ©es.

Transformation : Structurer les donnÃ©es pour l'analyse avec un schÃ©ma en Ã©toile.

Stockage : Assurer un stockage fiable dans PostgreSQL et Delta Lake.

Monitoring : ContrÃ´ler la qualitÃ© des donnÃ©es et alerter en cas d'anomalie.

Orchestration : Coordonner les tÃ¢ches du pipeline avec Prefect.

Planification : Automatiser les exÃ©cutions Ã  intervalles rÃ©guliers via Cron.

DÃ©ploiement : Conteneuriser l'application avec Docker pour simplifier l'intÃ©gration.

Tests : Valider la fiabilitÃ© des scripts grÃ¢ce Ã  des tests unitaires.

ğŸ— Architecture du Projet

ModÃ¨le en Ã‰toile

Le pipeline utilise un modÃ¨le en Ã©toile pour faciliter les requÃªtes analytiques complexes et amÃ©liorer la performance.

Type

Description

Faits

Transactions, crÃ©dits, paiements, revenus, fraudes

Dimensions

Clients, produits, dates, comptes, satisfaction

ğŸ“‚ Structure du Projet

IngÃ©nierie_des_donnÃ©es_BI/
|
â”œâ”€â”€ donnees/
â”‚   â”œâ”€â”€ raw/ # DonnÃ©es brutes collectÃ©es
â”‚   â”œâ”€â”€ processed/ # DonnÃ©es nettoyÃ©es prÃªtes Ã  Ãªtre transformÃ©es
â”‚   â””â”€â”€ transformed/ # DonnÃ©es finales prÃªtes pour l'analyse
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py # Importation et prÃ©traitement des donnÃ©es
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ transformation/
â”‚   â”‚   â”œâ”€â”€ cleaning.py # Suppression des doublons et gestion des valeurs manquantes
â”‚   â”‚   â”œâ”€â”€ aggregation.py # Calculs et mÃ©triques (totaux, moyennes)
â”‚   â”‚   â”œâ”€â”€ enrichment.py # Jointures et enrichissement des donnÃ©es
â”‚   â”‚   â”œâ”€â”€ star_schema.py # Structuration du modÃ¨le en Ã©toile
â”‚   â”‚   â”œâ”€â”€ validation.py # RÃ¨gles de validation pour garantir la cohÃ©rence
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ stockage/
â”‚   â”‚   â”œâ”€â”€ database_storage.py # Stockage des donnÃ©es dans PostgreSQL
â”‚   â”‚   â”œâ”€â”€ delta_lake.py # Sauvegarde incrÃ©mentale dans Delta Lake
â”‚   â”‚   â””â”€â”€ test_connexion.py # VÃ©rification des connexions
â”‚   â”‚
â”‚   â”œâ”€â”€ surveillance/
â”‚   â”‚   â”œâ”€â”€ data_quality.py # Analyse de la qualitÃ© des donnÃ©es
â”‚   â”‚   â”œâ”€â”€ alerts.py # CrÃ©ation d'alertes automatisÃ©es
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â””â”€â”€ orchestration/
â”‚       â”œâ”€â”€ orchestration.py # DÃ©finition des flux avec Prefect
â”‚       â””â”€â”€ test_orchestration.py # Tests unitaires sur l'orchestration
â”‚
â”œâ”€â”€ tests/ # Tests unitaires pour chaque module
â”œâ”€â”€ docker-compose.yml # Configuration de Docker
â”œâ”€â”€ deployment.yaml # DÃ©ploiement des flux Prefect
â”œâ”€â”€ prefectignore # Exclusions spÃ©cifiques pour Prefect
â”œâ”€â”€ README.md # Documentation du projet
â””â”€â”€ requirements.txt # Liste des dÃ©pendances Python

ğŸ”‘ Ã‰tapes ClÃ©s

1. Ingestion de DonnÃ©es

Script : data_ingestion.py

Collecte des donnÃ©es brutes depuis CSV, API, ou autres sources.

Suppression des colonnes inutiles.

Conversion des types pour standardiser les formats.

2. Transformation des DonnÃ©es

Nettoyage des doublons et valeurs nulles (cleaning.py).

Calcul des mÃ©triques comme les totaux et moyennes (aggregation.py).

Fusion des sources pour enrichir les donnÃ©es (enrichment.py).

CrÃ©ation d'un modÃ¨le en Ã©toile (star_schema.py).

3. Orchestration et DÃ©ploiement

Orchestration des tÃ¢ches avec Prefect (orchestration.py).

Conteneurisation avec Docker Compose (docker-compose.yml).

ğŸ§ª Tests et DÃ©ploiement

Tests Unitaires

ExÃ©cutez les tests pour valider la fiabilitÃ© de chaque module :

pytest tests/

DÃ©ploiement avec Docker

Lancez l'application avec :

docker-compose up

ğŸŒŸ 

ğŸ Conclusion

Ce pipeline constitue une solution robuste et scalable pour les besoins analytiques des banques. En intÃ©grant des outils modernes comme Prefect et Docker, il garantit un traitement fiable des donnÃ©es critiques avec une supervision optimale.


