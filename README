Pipeline d'Ingénierie de Données pour l'Analyse Bancaire

Ce projet propose une solution complète d'ingénierie des données pour le secteur bancaire, permettant l'analyse approfondie des domaines critiques comme les transactions, les crédits, les fraudes et les profils clients. Grâce à une architecture en étoile, une orchestration automatisée avec Prefect, et un déploiement via Docker, ce pipeline garantit robustesse, scalabilité et efficacité.

Objectifs du projet

Ce pipeline a été conçu pour répondre aux besoins d'analyse des banques :

Ingestion : Automatiser la collecte et le prétraitement des données.

Transformation : Structurer les données pour l'analyse avec un schéma en étoile.

Stockage : Assurer un stockage fiable dans PostgreSQL et Delta Lake.

Monitoring : Contrôler la qualité des données et alerter en cas d'anomalie.

Orchestration : Coordonner les tâches du pipeline avec Prefect.

Planification : Automatiser les exécutions à intervalles réguliers via Cron.

Déploiement : Conteneuriser l'application avec Docker pour simplifier l'intégration.

Tests : Valider la fiabilité des scripts grâce à des tests unitaires.

Architecture du Projet

Modèle en Étoile

Le pipeline utilise un modèle en étoile pour faciliter les requêtes analytiques complexes et améliorer la performance. Les données sont organisées comme suit :

Tableaux de Faits

fact_transactions : Historique des transactions bancaires.

fact_loans : Données relatives aux crédits octroyés.

fact_payments : Paiements associés aux crédits et transactions.

fact_revenue : Suivi des revenus financiers.

fact_fraudes : Détection et signalement des activités frauduleuses.

Tableaux de Dimensions

dim_clients : Informations détaillées sur les clients.

dim_products : Liste des produits bancaires disponibles.

dim_dates : Structure temporelle pour l'analyse des périodes.

dim_accounts : Détails des comptes associés aux clients.

dim_satisfaction : Retour des clients pour l'amélioration des services.

Structure du projet

Ingénierie_des_données_BI/
|
├── donnees/
│   ├── raw/ # Données brutes collectées
│   ├── processed/ # Données nettoyées prêtes à être transformées
│   └── transformed/ # Données finales prêtes pour l'analyse
│
├── src/
│   ├── ingestion/
│   │   ├── data_ingestion.py # Importation et prétraitement des données
│   │   └── __init__.py
│   │
│   ├── transformation/
│   │   ├── cleaning.py # Suppression des doublons et gestion des valeurs manquantes
│   │   ├── aggregation.py # Calculs et métriques (totaux, moyennes)
│   │   ├── enrichment.py # Jointures et enrichissement des données
│   │   ├── star_schema.py # Structuration du modèle en étoile
│   │   ├── validation.py # Règles de validation pour garantir la cohérence
│   │   └── __init__.py
│   │
│   ├── stockage/
│   │   ├── database_storage.py # Stockage des données dans PostgreSQL
│   │   ├── delta_lake.py # Sauvegarde incrémentale dans Delta Lake
│   │   └── test_connexion.py # Vérification des connexions
│   │
│   ├── surveillance/
│   │   ├── data_quality.py # Analyse de la qualité des données
│   │   ├── alerts.py # Création d'alertes automatisées
│   │   └── __init__.py
│   │
│   └── orchestration/
│       ├── orchestration.py # Définition des flux avec Prefect
│       └── test_orchestration.py # Tests unitaires sur l'orchestration
│
├── tests/ # Tests unitaires pour chaque module
│   ├── test_ingestion.py
│   ├── test_transformation.py
│   └── test_storage.py
│
├── docker-compose.yml # Configuration de Docker
├── deployment.yaml # Déploiement des flux Prefect
├── prefectignore # Exclusions spécifiques pour Prefect
├── README.md # Documentation du projet
└── requirements.txt # Liste des dépendances Python

Étapes Clés

1. Ingestion de Données

Script : data_ingestion.py

Objectif : Collecter les données brutes depuis CSV, API, ou autres sources.

Actions :

Chargement des données.

Suppression des colonnes inutiles.

Conversion des types pour standardiser les formats.

2. Transformation des Données

Scripts principaux :

cleaning.py : Nettoyage des doublons et valeurs nulles.

aggregation.py : Calcul des métriques (totaux, moyennes).

enrichment.py : Fusion et enrichissement des données.

star_schema.py : Création du modèle en étoile.

3. Stockage

Scripts :

database_storage.py : Stockage dans PostgreSQL.

delta_lake.py : Intégration dans Delta Lake.

4. Orchestration avec Prefect

Script : orchestration.py

Fonctionnalités : Automation des tâches avec Prefect.

Tests et Déploiement

Tests Unitaires

Exécutez les tests pour valider la fiabilité de chaque module :

pytest tests/

Déploiement avec Docker

Utilisez docker-compose pour déployer le projet :

docker-compose up

Conclusion

Ce pipeline constitue une solution robuste et scalable pour les besoins analytiques des banques. En intégrant des outils modernes comme Prefect et Docker, il permet un traitement fiable des données critiques.





